{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5237b-bff8-4700-a414-6eea9e494e39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"checking if the kernel is working\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682e60d-fb8a-4d75-a681-2d3b6267d982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y numpy --quiet\n",
    "!pip uninstall -y tensorflow --quiet\n",
    "!pip install tensorflow==2.17 --quiet\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba89558-8e61-4792-bc8d-57173dda4fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__, \"must be 2.17.0\")\n",
    "import numpy as np\n",
    "print(np.__version__, \"must be 1.26.4\")\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "import random, os, sys\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fef53c-16aa-4bee-b751-5b28b7804079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def average_wavelengths(chip, wavelength_indices):\n",
    "    out_chip = chip[:,:,wavelength_indices]\n",
    "    out_chip = np.nanmean(out_chip, axis=2)\n",
    "    return out_chip\n",
    "\n",
    "def convert_chip_to_different_spectra(chip, in_wavelengths, out_wavelengths, visualize=False):\n",
    "    out_chip = []\n",
    "    for out_wavelength_index in range(len(out_wavelengths)):\n",
    "        out_wavelength_start = out_wavelengths[out_wavelength_index][0]\n",
    "        out_wavelength_end = out_wavelengths[out_wavelength_index][1]\n",
    "        \n",
    "        in_wavelengths_to_grab = []\n",
    "        for i in range(len(in_wavelengths)):\n",
    "            in_wavelength = in_wavelengths[i]\n",
    "            if in_wavelength > out_wavelength_start and in_wavelength < out_wavelength_end:\n",
    "                in_wavelengths_to_grab.append(i)\n",
    "        \n",
    "        new_chip_at_wavelegnth = average_wavelengths(chip, in_wavelengths_to_grab)\n",
    "        #print(new_chip_at_wavelegnth.shape)\n",
    "        out_chip.append(new_chip_at_wavelegnth)\n",
    "        \n",
    "    out_chip = np.asarray(out_chip)\n",
    "    out_chip = np.moveaxis(out_chip,0,2)\n",
    "    #print(out_chip.shape)\n",
    "    \n",
    "    if visualize:\n",
    "        print(chip.shape)\n",
    "        print(out_chip.shape)\n",
    "        print(out_chip[0,0,:].shape)\n",
    "        \n",
    "        num_wavelengths = 100\n",
    "        fig = plt.figure(figsize = (20, 8))\n",
    "        plt.bar(in_wavelengths[0:num_wavelengths], chip[0,0,0:num_wavelengths])\n",
    "        plt.xlabel(\"Wavelength\")\n",
    "        plt.ylabel(\"Pixel Intensity (0.0-1.0)\")\n",
    "        plt.title(\"Intensities in one pixel of EMIT data, first 100 of 285 wavelengths\")\n",
    "        plt.show()\n",
    "        \n",
    "        num_wavelengths = len(out_wavelengths)\n",
    "        if num_wavelengths == 4:\n",
    "            string_wavelengths = [\"PS2 Blue\\n (455-515nm)\", \"PS2 Green\\n (500-590nm)\", \"PS2 Red\\n (590-670nm)\", \"PS2 NIR\\n (780-860nm)\"]\n",
    "            fig = plt.figure(figsize = (6, 6))\n",
    "            plt.bar(string_wavelengths, out_chip[0,0,:])\n",
    "            plt.xlabel(\"Wavelength\")\n",
    "            plt.ylabel(\"Pixel Intensity (0.0-1.0)\")\n",
    "            plt.title(\"Intensities in one pixel of synthetically constructed First Generation Planetscope Data\")\n",
    "            plt.show()\n",
    "            \n",
    "        if num_wavelengths == 8:\n",
    "            string_wavelengths = [\"PSB.SD Coastal Blue\\n (431-452nm)\", \"PSB.SD Blue\\n (465-515nm)\", \"PSB.SD Green 1\\n (513-549nm)\", \"PSB.SD Green 2\\n (547-583nm)\", \"PSB.SD Yellow\\n (600-620nm)\", \"PSB.SD Red\\n (650-680nm)\", \"PSB.SD Red-Edge\\n (697-713nm)\", \"PSB.SD NIR\\n (845-885nm)\", ]\n",
    "            fig = plt.figure(figsize = (14, 6))\n",
    "            plt.bar(string_wavelengths, out_chip[0,0,:])\n",
    "            plt.xlabel(\"Wavelength\")\n",
    "            plt.ylabel(\"Pixel Intensity (0.0-1.0)\")\n",
    "            plt.title(\"Intensities in one pixel of synthetically constructed Third Generation Planetscope Data\")\n",
    "            plt.show()\n",
    "    \n",
    "    #PS2 = First Generation aka Dove\n",
    "    #PS2.SD = Second Generation aka Dove-R\n",
    "    #PSB.SD = Third Generation Planetscope aka SuperDove\n",
    "    return out_chip\n",
    "\n",
    "def convert_emit_chip_to_planetscope_chip(emit_chip_filename, planetscope_chip_filename, emit_wavelengths, planetscope_wavelengths, do_save=True, visualize=False):\n",
    "    in_chip = np.load(emit_chip_filename)\n",
    "    out_chip = convert_chip_to_different_spectra(in_chip, emit_wavelengths, planetscope_wavelengths, visualize=visualize)\n",
    "    if do_save: np.save(planetscope_chip_filename, out_chip)\n",
    "    \n",
    "def convert_whole_dataset(emit_chip_directory, planetscope_chip_directory, emit_wavelengths, planetscope_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=0, do_save=True, visualize=False):\n",
    "    if not os.path.exists(planetscope_chip_directory):\n",
    "        os.mkdir(planetscope_chip_directory)\n",
    "        \n",
    "    in_chip_paths = glob(os.path.join(emit_chip_directory,'*'))\n",
    "    for chip_index, in_chip_path in enumerate(in_chip_paths):\n",
    "        out_filename = chipname_prefix + f\"{chipname_start_index+chip_index:05}\" + \".npy\"\n",
    "        out_path = os.path.join(planetscope_chip_directory, out_filename)\n",
    "        \n",
    "        convert_emit_chip_to_planetscope_chip(in_chip_path, out_path, emit_wavelengths, planetscope_wavelengths, do_save=do_save, visualize=visualize)\n",
    "        if visualize:\n",
    "            break\n",
    "            \n",
    "        \n",
    "#Src: https://assets.planet.com/docs/Planet_PSScene_Imagery_Product_Spec_letter_screen.pdf Pg 10\n",
    "planetscope_old_wavelengths = [(455, 515), (500, 590), (590, 670), (780,860)]\n",
    "planetscope_new_wavelengths = [(431,452), (465,515), (513,549), (547,583), (600,620), (650,680), (697,713), (845,885)]\n",
    "        \n",
    "emit_wavelengths = [ 381.00558,  388.4092 ,  395.81583,  403.2254 ,  410.638  ,\n",
    "        418.0536 ,  425.47214,  432.8927 ,  440.31726,  447.7428 ,\n",
    "        455.17035,  462.59888,  470.0304 ,  477.46292,  484.89743,\n",
    "        492.33292,  499.77142,  507.2099 ,  514.6504 ,  522.0909 ,\n",
    "        529.5333 ,  536.9768 ,  544.42126,  551.8667 ,  559.3142 ,\n",
    "        566.7616 ,  574.20905,  581.6585 ,  589.108  ,  596.55835,\n",
    "        604.0098 ,  611.4622 ,  618.9146 ,  626.36804,  633.8215 ,\n",
    "        641.2759 ,  648.7303 ,  656.1857 ,  663.6411 ,  671.09753,\n",
    "        678.5539 ,  686.0103 ,  693.4677 ,  700.9251 ,  708.38354,\n",
    "        715.84094,  723.2993 ,  730.7587 ,  738.2171 ,  745.6765 ,\n",
    "        753.1359 ,  760.5963 ,  768.0557 ,  775.5161 ,  782.97754,\n",
    "        790.4379 ,  797.89935,  805.36176,  812.8232 ,  820.2846 ,\n",
    "        827.746  ,  835.2074 ,  842.66986,  850.1313 ,  857.5937 ,\n",
    "        865.0551 ,  872.5176 ,  879.98004,  887.44147,  894.90393,\n",
    "        902.3664 ,  909.82886,  917.2913 ,  924.7538 ,  932.21625,\n",
    "        939.6788 ,  947.14026,  954.6027 ,  962.0643 ,  969.5268 ,\n",
    "        976.9883 ,  984.4498 ,  991.9114 ,  999.37286, 1006.8344 ,\n",
    "       1014.295  , 1021.7566 , 1029.2172 , 1036.6777 , 1044.1383 ,\n",
    "       1051.5989 , 1059.0596 , 1066.5201 , 1073.9797 , 1081.4404 ,\n",
    "       1088.9    , 1096.3597 , 1103.8184 , 1111.2781 , 1118.7368 ,\n",
    "       1126.1964 , 1133.6552 , 1141.1129 , 1148.5717 , 1156.0304 ,\n",
    "       1163.4882 , 1170.9459 , 1178.4037 , 1185.8616 , 1193.3184 ,\n",
    "       1200.7761 , 1208.233  , 1215.6898 , 1223.1467 , 1230.6036 ,\n",
    "       1238.0596 , 1245.5154 , 1252.9724 , 1260.4283 , 1267.8833 ,\n",
    "       1275.3392 , 1282.7942 , 1290.2502 , 1297.7052 , 1305.1603 ,\n",
    "       1312.6144 , 1320.0685 , 1327.5225 , 1334.9756 , 1342.4287 ,\n",
    "       1349.8818 , 1357.3351 , 1364.7872 , 1372.2384 , 1379.6907 ,\n",
    "       1387.1418 , 1394.5931 , 1402.0433 , 1409.4937 , 1416.944  ,\n",
    "       1424.3933 , 1431.8427 , 1439.292  , 1446.7404 , 1454.1888 ,\n",
    "       1461.6372 , 1469.0847 , 1476.5321 , 1483.9796 , 1491.4261 ,\n",
    "       1498.8727 , 1506.3192 , 1513.7649 , 1521.2104 , 1528.655  ,\n",
    "       1536.1007 , 1543.5454 , 1550.9891 , 1558.4329 , 1565.8766 ,\n",
    "       1573.3193 , 1580.7621 , 1588.205  , 1595.6467 , 1603.0886 ,\n",
    "       1610.5295 , 1617.9705 , 1625.4104 , 1632.8513 , 1640.2903 ,\n",
    "       1647.7303 , 1655.1694 , 1662.6074 , 1670.0455 , 1677.4836 ,\n",
    "       1684.9209 , 1692.358  , 1699.7952 , 1707.2314 , 1714.6667 ,\n",
    "       1722.103  , 1729.5383 , 1736.9727 , 1744.4071 , 1751.8414 ,\n",
    "       1759.2749 , 1766.7084 , 1774.1418 , 1781.5743 , 1789.007  ,\n",
    "       1796.4385 , 1803.8701 , 1811.3008 , 1818.7314 , 1826.1611 ,\n",
    "       1833.591  , 1841.0206 , 1848.4495 , 1855.8773 , 1863.3052 ,\n",
    "       1870.733  , 1878.16   , 1885.5869 , 1893.013  , 1900.439  ,\n",
    "       1907.864  , 1915.2892 , 1922.7133 , 1930.1375 , 1937.5607 ,\n",
    "       1944.9839 , 1952.4071 , 1959.8295 , 1967.2518 , 1974.6732 ,\n",
    "       1982.0946 , 1989.515  , 1996.9355 , 2004.355  , 2011.7745 ,\n",
    "       2019.1931 , 2026.6118 , 2034.0304 , 2041.4471 , 2048.865  ,\n",
    "       2056.2808 , 2063.6965 , 2071.1123 , 2078.5273 , 2085.9421 ,\n",
    "       2093.3562 , 2100.769  , 2108.1821 , 2115.5942 , 2123.0063 ,\n",
    "       2130.4175 , 2137.8289 , 2145.239  , 2152.6482 , 2160.0576 ,\n",
    "       2167.467  , 2174.8755 , 2182.283  , 2189.6904 , 2197.097  ,\n",
    "       2204.5034 , 2211.9092 , 2219.3147 , 2226.7195 , 2234.1233 ,\n",
    "       2241.5269 , 2248.9297 , 2256.3328 , 2263.7346 , 2271.1365 ,\n",
    "       2278.5376 , 2285.9387 , 2293.3386 , 2300.7378 , 2308.136  ,\n",
    "       2315.5342 , 2322.9326 , 2330.3298 , 2337.7263 , 2345.1216 ,\n",
    "       2352.517  , 2359.9126 , 2367.3071 , 2374.7007 , 2382.0935 ,\n",
    "       2389.486  , 2396.878  , 2404.2695 , 2411.6604 , 2419.0513 ,\n",
    "       2426.4402 , 2433.8303 , 2441.2183 , 2448.6064 , 2455.9944 ,\n",
    "       2463.3816 , 2470]\n",
    "    \n",
    "emit_dataset_path_0 = \"./reflectance/\"\n",
    "emit_dataset_path_1 = \"./reflectance_1/\"\n",
    "emit_dataset_path_2 = \"./reflectance_2/\"\n",
    "emit_dataset_path_3 = \"./reflectance_3/\"\n",
    "emit_dataset_path_4 = \"./reflectance_4/\"\n",
    "emit_dataset_path_5 = \"./reflectance_5/\"\n",
    "\n",
    "if True:\n",
    "    convert_whole_dataset(emit_dataset_path_3, \"./dataset_for_cnn/train/x/\", emit_wavelengths, planetscope_old_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=100000, do_save=False, visualize=True)\n",
    "    convert_whole_dataset(emit_dataset_path_3, \"./dataset_for_cnn/train/x/\", emit_wavelengths, planetscope_new_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=100000, do_save=False, visualize=True)\n",
    "\n",
    "if False: #manual toggle since this only needs to happen once\n",
    "    #Granule 0:\n",
    "    print(\"Processing granule 0\")\n",
    "    convert_whole_dataset(emit_dataset_path_0, \"./dataset_for_cnn/train/x/\", emit_wavelengths, planetscope_old_wavelengths, chipname_prefix = \"chip_\")\n",
    "    convert_whole_dataset(emit_dataset_path_0, \"./dataset_for_cnn/train/y/\", emit_wavelengths, planetscope_new_wavelengths, chipname_prefix = \"chip_\")\n",
    "    print(\"Granule 0 processed into train data\")\n",
    "\n",
    "    #Granule 1:\n",
    "    print(\"Processing granule 1\")\n",
    "    convert_whole_dataset(emit_dataset_path_1, \"./dataset_for_cnn/test/x/\", emit_wavelengths, planetscope_old_wavelengths, chipname_prefix = \"chip_\")\n",
    "    convert_whole_dataset(emit_dataset_path_1, \"./dataset_for_cnn/test/y/\", emit_wavelengths, planetscope_new_wavelengths, chipname_prefix = \"chip_\")\n",
    "    print(\"Granule 1 processed into test data\")\n",
    "\n",
    "    # #Granule 2:\n",
    "    print(\"Processing granule 2\")\n",
    "    convert_whole_dataset(emit_dataset_path_2, \"./dataset_for_cnn/valid/x/\", emit_wavelengths, planetscope_old_wavelengths, chipname_prefix = \"chip_\")\n",
    "    convert_whole_dataset(emit_dataset_path_2, \"./dataset_for_cnn/valid/y/\", emit_wavelengths, planetscope_new_wavelengths, chipname_prefix = \"chip_\")\n",
    "    print(\"Granule 2 processed into valid data\")\n",
    "    \n",
    "    # #Granule 3:\n",
    "    print(\"Processing granule 3\")\n",
    "    convert_whole_dataset(emit_dataset_path_3, \"./dataset_for_cnn/train/x/\", emit_wavelengths, planetscope_old_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=10000)\n",
    "    convert_whole_dataset(emit_dataset_path_3, \"./dataset_for_cnn/train/y/\", emit_wavelengths, planetscope_new_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=10000)\n",
    "    print(\"Granule 3 processed into train data\")\n",
    "    \n",
    "    # #Granule 4:\n",
    "    print(\"Processing granule 4\")\n",
    "    convert_whole_dataset(emit_dataset_path_4, \"./dataset_for_cnn/train/x/\", emit_wavelengths, planetscope_old_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=20000)\n",
    "    convert_whole_dataset(emit_dataset_path_4, \"./dataset_for_cnn/train/y/\", emit_wavelengths, planetscope_new_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=20000)\n",
    "    print(\"Granule 4 processed into train data\")\n",
    "    \n",
    "    # #Granule 5:\n",
    "    print(\"Processing granule 5\")\n",
    "    convert_whole_dataset(emit_dataset_path_5, \"./dataset_for_cnn/train/x/\", emit_wavelengths, planetscope_old_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=30000)\n",
    "    convert_whole_dataset(emit_dataset_path_5, \"./dataset_for_cnn/train/y/\", emit_wavelengths, planetscope_new_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=30000)\n",
    "    print(\"Granule 5 processed into train data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4d0e0-ef93-46b6-a19d-4ec1ae63acac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, x_chip_path_dir, y_chip_path_dir, batch_size, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.x_chip_path_dir = x_chip_path_dir\n",
    "        self.y_chip_path_dir = y_chip_path_dir\n",
    "        \n",
    "        self.x_chip_path_list = glob(os.path.join(self.x_chip_path_dir,'*'))\n",
    "        self.y_chip_path_list = glob(os.path.join(self.y_chip_path_dir,'*'))\n",
    "        \n",
    "        self.num_files = len(self.x_chip_path_list)\n",
    "        self.index_path = list(range(self.num_files))+list(range(self.num_files))\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        if self.shuffle == True:\n",
    "            random.shuffle(self.index_path)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.index_path)\n",
    "    \n",
    "    def __getitem__(self, index, debug=False):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in range(self.batch_size):\n",
    "            if debug: print(i, \"Index:\", self.index_path[i+index], \"Path:\", self.y_chip_path_list[self.index_path[i+index]])\n",
    "            #if debug: print(self.index_path)\n",
    "            x = np.load(self.x_chip_path_list[self.index_path[i+index]])\n",
    "            y = np.load(self.y_chip_path_list[self.index_path[i+index]])\n",
    "            \n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "        \n",
    "        X = np.asarray(X).astype('float32')\n",
    "        Y = np.asarray(Y).astype('float32')\n",
    "       \n",
    "        \n",
    "       # # print(np.isnan(np.min(X)))\n",
    "       #  x = np.asarray(x).astype('float32')\n",
    "       #  x = np.zeros((1, 64, 64, 4))\n",
    "       #  y = np.zeros((1,64,64,8))\n",
    "       #  print(x)\n",
    "        # X = tf.convert_to_tensor(X.tolist())\n",
    "        # Y = tf.convert_to_tensor(Y.tolist())\n",
    "        \n",
    "        return X, Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_files // self.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58bea20-e21a-4eb5-a889-fcb493a080f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_fcn_model(num_hidden_layers=3, first_layer_num_filters=32, first_layer_kernel_size=3, hidden_layer_num_filters=64, hidden_layer_kernel_size=3, layer_activation='sigmoid'):\n",
    "    # num_hidden_layers must be > 1\n",
    "    # *_num_filters must be positive integer - recommend 8, 16, 32, 64, etc\n",
    "    # *_kernel_size must be odd positive integer - recommend 1, 3, 5, 7, 9, etc\n",
    "    # see documentation for tf.keras for different activations\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(first_layer_num_filters, (first_layer_kernel_size, first_layer_kernel_size), activation=layer_activation, padding='same'))\n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(tf.keras.layers.Conv2D(hidden_layer_num_filters, (hidden_layer_kernel_size, hidden_layer_kernel_size), activation=layer_activation, padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='sigmoid', padding='same'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "#adapted from: https://github.com/oekosheri/tensorflow_unet_scaling/blob/main/models.py\n",
    "def conv_block(input, num_filters, conv_block_size=2, do_batchnorm=True, activation=\"relu\"):\n",
    "    x = tf.keras.layers.Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    if do_batchnorm == True:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)  # Not in the original network.\n",
    "    x = tf.keras.layers.Activation(activation)(x)\n",
    "    \n",
    "    for i in range(conv_block_size-1):\n",
    "        x = tf.keras.layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "        if do_batchnorm == True:\n",
    "            x = tf.keras.layers.BatchNormalization()(x)  # Not in the original network\n",
    "        x = tf.keras.layers.Activation(activation)(x)\n",
    "        \n",
    "    return x\n",
    "\n",
    "def encoder_block(input, num_filters, conv_block_size=2, do_batchnorm=True, activation=\"relu\"):\n",
    "    x = conv_block(input, num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "    p = tf.keras.layers.MaxPool2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters, conv_block_size=2, do_batchnorm=True, activation=\"relu\"):\n",
    "    \n",
    "    x = tf.keras.layers.Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "    return x\n",
    "\n",
    "def generate_unet_model(unet_depth=3, num_filters_base=8, conv_block_size=2, do_batchnorm=True, activation=\"relu\"):\n",
    "    inputs = tf.keras.layers.Input((64,64,4)) #Assuming 64x64x4 chipsize\n",
    "    \n",
    "    s_list = []\n",
    "    p_list = []\n",
    "    d_list = []\n",
    "    num_filters = num_filters_base    \n",
    "    \n",
    "    #encoding steps\n",
    "    for i in range(unet_depth):\n",
    "        if i == 0:\n",
    "            s, p = encoder_block(inputs, num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "        else:\n",
    "            s, p = encoder_block(p_list[i-1], num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "        s_list.append(s)\n",
    "        p_list.append(p)\n",
    "        num_filters *= 2\n",
    "        \n",
    "    #bridge\n",
    "    b1 = conv_block(p_list[-1], num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "    num_filters = int(num_filters/2)\n",
    "    \n",
    "    #decoding steps:\n",
    "    for i in range(unet_depth):\n",
    "        if i == 0:\n",
    "            d = decoder_block(b1, s_list[-1-i], num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "        else:\n",
    "            d = decoder_block(d_list[i-1], s_list[-1-i], num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "            \n",
    "        d_list.append(d)\n",
    "        num_filters = int(num_filters/2)\n",
    "    \n",
    "    #head\n",
    "    output = tf.keras.layers.Conv2D(8, (3, 3), activation=activation, padding='same')(d_list[-1])\n",
    "    model = tf.keras.models.Model(inputs, output, name=\"U-Net\")\n",
    "    return model\n",
    "\n",
    "\n",
    "#adapted from: https://www.analyticsvidhya.com/blog/2021/08/how-to-code-your-resnet-from-scratch-in-tensorflow/\n",
    "def identity_block(x, num_filters):\n",
    "    # copy tensor to variable called x_skip\n",
    "    x_skip = x\n",
    "    # Layer 1\n",
    "    x = tf.keras.layers.Conv2D(num_filters, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    # Layer 2\n",
    "    x = tf.keras.layers.Conv2D(num_filters, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    # Add Residue\n",
    "    x = tf.keras.layers.Add()([x, x_skip])     \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def convolutional_block(x, num_filters):\n",
    "    # copy tensor to variable called x_skip\n",
    "    x_skip = x\n",
    "    # Layer 1\n",
    "    x = tf.keras.layers.Conv2D(num_filters, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    # Layer 2\n",
    "    x = tf.keras.layers.Conv2D(num_filters, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    # Processing Residue with conv(1,1)\n",
    "    x_skip = tf.keras.layers.Conv2D(num_filters, (1,1))(x_skip)\n",
    "    # Add Residue\n",
    "    x = tf.keras.layers.Add()([x, x_skip])     \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def generate_resnet_model(num_filters=64, block_layers=[3,4,6,3]):\n",
    "    # Step 1 (Setup Input Layer)\n",
    "    x_input = tf.keras.layers.Input((64,64,4)) #Assuming 64x64x4 chipsize\n",
    "\n",
    "    # Step 2 (Initial Conv layer along with maxPool)\n",
    "    x = tf.keras.layers.Conv2D(num_filters, kernel_size=3, padding='same')(x_input)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    # Define size of sub-blocks and initial filter size\n",
    "    # block_layers = [3, 4, 6, 3]\n",
    "    # filter_size = 64\n",
    "    # Step 3 Add the Resnet Blocks\n",
    "    for i in range(len(block_layers)):\n",
    "        if i == 0:\n",
    "            # For sub-block 1 Residual/Convolutional block not needed\n",
    "            for j in range(block_layers[i]):\n",
    "                x = identity_block(x, num_filters)\n",
    "        else:\n",
    "            # One Residual/Convolutional Block followed by Identity blocks\n",
    "            x = convolutional_block(x, num_filters)\n",
    "            for j in range(block_layers[i] - 1):\n",
    "                x = identity_block(x, num_filters)\n",
    "                \n",
    "    # head\n",
    "    output = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    model = tf.keras.models.Model(x_input, output, name=\"ResNet\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf54b58-bdf8-4319-bfdb-efcc525bab08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, batch_size=1, num_epochs=10, learning_rate=1e-3, model_checkpoint_directory=\"./saved_models/temp/\", model_checkpoint_filename=\"temp_current_best.keras\"):\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=tf.keras.losses.MeanAbsoluteError(), metrics=['accuracy', 'root_mean_squared_error'])\n",
    "    #print(model.summary())\n",
    "    training_generator = CustomDataGenerator(\"./dataset_for_cnn/train/x/\", \"./dataset_for_cnn/train/y/\", batch_size)\n",
    "    validation_generator = CustomDataGenerator(\"./dataset_for_cnn/valid/x/\", \"./dataset_for_cnn/valid/y/\", batch_size)\n",
    "    early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=20, start_from_epoch=40, verbose=1)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                    filepath=os.path.join(model_checkpoint_directory,model_checkpoint_filename),\n",
    "                                    monitor='val_loss',\n",
    "                                    mode='min',\n",
    "                                    save_best_only=True)\n",
    "\n",
    "    \n",
    "    history = model.fit(training_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        epochs=num_epochs,\n",
    "                        callbacks=[early_stop_callback, model_checkpoint_callback]\n",
    "                        )\n",
    "    return model, history\n",
    "    \n",
    "def score_one_datapoint(y, y_pred):\n",
    "    psnr = skimage.metrics.peak_signal_noise_ratio(np.asarray(y), y_pred)\n",
    "    ssim = skimage.metrics.structural_similarity(np.asarray(y), y_pred, data_range = 1)\n",
    "    mse = skimage.metrics.mean_squared_error(np.asarray(y), y_pred)\n",
    "    return psnr, ssim, mse\n",
    "    \n",
    "def test_and_score_model(model, verbose=False, num_samples=32, debug=False, return_all_bands=False, visualize_results=False, visualize_index=0, visualize_band=0):\n",
    "    testing_generator = CustomDataGenerator(\"./dataset_for_cnn/test/x/\", \"./dataset_for_cnn/test/y/\", num_samples)\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    mse_list = []\n",
    "    \n",
    "    print(\"Testing and Scoring Model...\")\n",
    "    \n",
    "    x, y = testing_generator.__getitem__(0, debug=False)\n",
    "    \n",
    "    \n",
    "    y_pred = model.predict(x, steps=num_samples, verbose=0)\n",
    "    if debug: print(y_pred.shape, y.shape)\n",
    "    \n",
    "    for band_index in range(8):\n",
    "        psnr_list.append([])\n",
    "        ssim_list.append([])\n",
    "        mse_list.append([])\n",
    "        \n",
    "        for image_index in range(num_samples):\n",
    "            psnr, ssim, mse = score_one_datapoint(y[image_index,:,:,band_index], y_pred[image_index,:,:,band_index])\n",
    "            psnr_list[band_index].append(psnr)\n",
    "            ssim_list[band_index].append(ssim)\n",
    "            mse_list[band_index].append(mse)\n",
    "    \n",
    "    if debug: print(np.asarray(psnr_list).shape)\n",
    "    avg_psnr = [np.mean(np.asarray(psnr_sublist)) for psnr_sublist in psnr_list]\n",
    "    avg_ssim = [np.mean(np.asarray(ssim_sublist)) for ssim_sublist in ssim_list]\n",
    "    avg_mse =  [np.mean(np.asarray(mse_sublist)) for mse_sublist in mse_list]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"PSB.SD Bands: Coastal Blue, Blue, Green 1, Green 2, Yellow, Red, Red-Edge, NIR\")\n",
    "        print(\"Average PSNR Per Band:\", *(f\"{x:.3f}\" for x in avg_psnr), \"All Bands:\", f\"{np.mean(avg_psnr):.3f}\")\n",
    "        print(\"Average SSIM Per Band:\", *(f\"{x:.3f}\" for x in avg_ssim), \"All Bands:\", f\"{np.mean(avg_ssim):.3f}\")\n",
    "        print(\"Average MSE Per Band:\", *(f\"{x:.5f}\" for x in avg_mse), \"All Bands:\", f\"{np.mean(avg_mse):.5f}\")\n",
    "        \n",
    "    if visualize_results:\n",
    "        print(\"y range:\", np.min(y), np.mean(y), np.max(y))\n",
    "        print(\"ypred  :\", np.min(y_pred), np.mean(y_pred), np.max(y_pred))\n",
    "        f, axarr = plt.subplots(2,8) \n",
    "        f.set_figheight(8)\n",
    "        f.set_figwidth(24)\n",
    "        for i in range(8):\n",
    "            ymin = min(np.min(y[visualize_index,:,:,i]), np.min(y_pred[visualize_index,:,:,i]))\n",
    "            ymax = max(np.max(y[visualize_index,:,:,i]), np.max(y_pred[visualize_index,:,:,i]))\n",
    "            axarr[0][i].imshow(y[visualize_index,:,:,i], vmin=ymin, vmax=ymax)\n",
    "            axarr[1][i].imshow(y_pred[visualize_index,:,:,i], vmin=ymin, vmax=ymax)\n",
    "    \n",
    "        \n",
    "    if debug:\n",
    "        return psnr_list, ssim_list, mse_list\n",
    "    elif return_all_bands:\n",
    "        return avg_psnr, avg_ssim, avg_mse\n",
    "    else:\n",
    "        return np.mean(avg_psnr), np.mean(avg_ssim), np.mean(avg_mse)\n",
    "    \n",
    "    \n",
    "def interrogate_data():\n",
    "    training_generator = CustomDataGenerator(\"./dataset_for_cnn/train/x/\", \"./dataset_for_cnn/train/y/\", 512)\n",
    "    validation_generator = CustomDataGenerator(\"./dataset_for_cnn/valid/x/\", \"./dataset_for_cnn/valid/y/\", 512)\n",
    "    testing_generator = CustomDataGenerator(\"./dataset_for_cnn/test/x/\", \"./dataset_for_cnn/test/y/\", 512)\n",
    "    \n",
    "    x, y = training_generator.__getitem__(0)\n",
    "    print(\"Training X:  Min:\", np.min(x), \"Max:\", np.max(x), \"Mean:\", np.mean(x, axis=(0,1,2)), \"shape:\", np.shape(x))\n",
    "    print(\"Training Y:  Min:\", np.min(y), \"Max:\", np.max(y), \"Mean:\", np.mean(y, axis=(0,1,2)), \"shape:\", np.shape(y))\n",
    "\n",
    "    x, y = validation_generator.__getitem__(0)\n",
    "    print(\"Validation X:  Min:\", np.min(x), \"Max:\", np.max(x), \"Mean:\", np.mean(x, axis=(0,1,2)), \"shape:\", np.shape(x))\n",
    "    print(\"Validation Y:  Min:\", np.min(y), \"Max:\", np.max(y), \"Mean:\", np.mean(y, axis=(0,1,2)), \"shape:\", np.shape(y))\n",
    "    \n",
    "    x, y = testing_generator.__getitem__(0)\n",
    "    print(\"Testing X:  Min:\", np.min(x), \"Max:\", np.max(x), \"Mean:\", np.mean(x, axis=(0,1,2)), \"shape:\", np.shape(x))\n",
    "    print(\"Testing Y:  Min:\", np.min(y), \"Max:\", np.max(y), \"Mean:\", np.mean(y, axis=(0,1,2)), \"shape:\", np.shape(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72b3b4-f9a9-4148-afbb-ddabc5eda7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_model_training(model=None, training_params=None, fcn_model_params=None, unet_model_params=None, resnet_model_params=None, lr=1e-4, model_save_dir=\"./saved_models/temp/\", model_type=\"fcn\", visualize=False):\n",
    "    if not os.path.exists(model_save_dir):\n",
    "            os.mkdir(model_save_dir)\n",
    "            \n",
    "    # Generate unique ID\n",
    "    unique_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    ## Training Parameters\n",
    "    #TODO: Play with these values first to see if you can get models to converge quickly\n",
    "    if training_params == None:\n",
    "        print(\"No training params given, reverting to defaults\")\n",
    "        batch_size = 32              #Normally you want to push this as high as you can, multiples of 2, until you run out of VRAM\n",
    "        num_epochs = 1000             #Increase to train more.  May need to be increased as batch size is increased to keep number of training steps equal\n",
    "        learning_rate = 1e-4        #Play with this to get models to converge faster\n",
    "    else:\n",
    "        batch_size = training_params['batch_size']\n",
    "        num_epochs = training_params['num_epochs']\n",
    "        learning_rate = training_params['learning_rate']\n",
    "        \n",
    "        \n",
    "    if model == None and model_type == \"fcn\":\n",
    "        ## FCN Model Parameters\n",
    "        #TODO: Grid-Search these, log PSNR, SSIM, MSE for each trained model in a spreadsheet:\n",
    "        if fcn_model_params == None:\n",
    "            print(\"No fcn params given, reverting to defaults\")\n",
    "            num_hidden_layers=3         #try 0,1,2,3,4,5,...,12     This will be the most important feature probably\n",
    "            first_layer_num_filters=32  #try 4,8,16,32,64,128       This might be important but can be dropped\n",
    "            first_layer_kernel_size=3   #try 1,3,5,7,9 must be odd  Least important, first to drop\n",
    "            hidden_layer_num_filters=64 #try 4,8,16,32,64,128       Probably the second most important feature\n",
    "            hidden_layer_kernel_size=3  #try 1,3,5,7,9 must be odd  Least important, first to drop\n",
    "            layer_activation='sigmoid'  #try sigmoid and relu       This might be important but can be dropped\n",
    "        else:\n",
    "            num_hidden_layers=fcn_model_params['num_hidden_layers']\n",
    "            first_layer_num_filters=fcn_model_params['first_layer_num_filters']\n",
    "            first_layer_kernel_size=fcn_model_params['first_layer_kernel_size']\n",
    "            hidden_layer_num_filters=fcn_model_params['hidden_layer_num_filters']\n",
    "            hidden_layer_kernel_size=fcn_model_params['hidden_layer_kernel_size']\n",
    "            layer_activation=fcn_model_params['layer_activation']\n",
    "            \n",
    "        ## Generate FCM\n",
    "        best_model_name = f\"fcm_bs{batch_size}_lr{learning_rate}_layers{num_hidden_layers}_filter1{first_layer_num_filters}_size1{first_layer_kernel_size}_filterN{hidden_layer_num_filters}_sizeN{hidden_layer_kernel_size}_activ{layer_activation}_{unique_id}_best.keras\"\n",
    "        final_model_name= f\"fcm_bs{batch_size}_lr{learning_rate}_layers{num_hidden_layers}_filter1{first_layer_num_filters}_size1{first_layer_kernel_size}_filterN{hidden_layer_num_filters}_sizeN{hidden_layer_kernel_size}_activ{layer_activation}_{unique_id}_final.keras\"\n",
    "        model = generate_fcn_model(num_hidden_layers=num_hidden_layers, \n",
    "                               first_layer_num_filters=first_layer_num_filters, \n",
    "                               first_layer_kernel_size=first_layer_kernel_size, \n",
    "                               hidden_layer_num_filters=hidden_layer_num_filters, \n",
    "                               hidden_layer_kernel_size=hidden_layer_kernel_size,\n",
    "                               layer_activation=layer_activation)\n",
    "            \n",
    "    elif model == None and model_type == \"unet\":\n",
    "        ## U-Net Model Parameters\n",
    "        #TODO: Grid-Search these, log PSNR, SSIM, MSE for each trained model in a spreadsheet:\n",
    "        if unet_model_params == None:\n",
    "            print(\"No fcn params given, reverting to defaults\")\n",
    "            unet_depth=3             #try 1,2,3,4,5,6,7,8\n",
    "            num_filters_base=8       #try 4,8,16,32\n",
    "            conv_block_size=2        #try 1,2,3,4,5,6,7,8\n",
    "            do_batchnorm=True        #try true/false\n",
    "            activation=\"relu\"        #try sigmoid and relu\n",
    "        else:\n",
    "            unet_depth=unet_model_params['unet_depth']\n",
    "            num_filters_base=unet_model_params['num_filters_base']\n",
    "            conv_block_size=unet_model_params['conv_block_size']\n",
    "            do_batchnorm=unet_model_params['do_batchnorm']\n",
    "            activation=unet_model_params['activation']\n",
    "\n",
    "        # Generate U-Net\n",
    "        best_model_name = f\"unet_bs{batch_size}_lr{learning_rate}_depth{unet_depth}_basefilters{num_filters_base}_cblocksize{conv_block_size}_BN{do_batchnorm}_activ{activation}_{unique_id}_best.keras\"\n",
    "        final_model_name= f\"unet_bs{batch_size}_lr{learning_rate}_depth{unet_depth}_basefilters{num_filters_base}_cblocksize{conv_block_size}_BN{do_batchnorm}_activ{activation}_{unique_id}_final.keras\"\n",
    "        model = generate_unet_model(unet_depth=unet_depth, \n",
    "                                    num_filters_base=num_filters_base, \n",
    "                                    conv_block_size=conv_block_size, \n",
    "                                    do_batchnorm=do_batchnorm, \n",
    "                                    activation=activation)\n",
    "        \n",
    "    elif model == None and model_type == \"resnet\":\n",
    "        ## Resnet Model Parameters:\n",
    "        if resnet_model_params == None:\n",
    "            num_filters = 64\n",
    "            block_layers = [2,2,2,2] #[3,4,6,3]\n",
    "        else:\n",
    "            num_filters = resnet_model_params['num_filters']\n",
    "            block_layers = resnet_model_params['block_layers']\n",
    "            \n",
    "        # Generate ResNet:\n",
    "        block_layers_string = ''\n",
    "        for blocksize in block_layers:\n",
    "            block_layers_string += str(blocksize)+'_'\n",
    "        best_model_name = f\"resnet_bs{batch_size}_lr{learning_rate}_num_filters{num_filters}_block_layers{block_layers_string}_{unique_id}_best.keras\"\n",
    "        final_model_name= f\"resnet_bs{batch_size}_lr{learning_rate}_num_filters{num_filters}_block_layers{block_layers_string}_{unique_id}_final.keras\"\n",
    "        \n",
    "        model = generate_resnet_model(num_filters=num_filters, block_layers=block_layers)\n",
    "        \n",
    "    elif model == None:\n",
    "        print(\"No model type given, must be fcn, unet, resnet\")\n",
    "    else:\n",
    "        print(\"Using existing model\")\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    ## Train Model\n",
    "    model, history = train_model(model, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, model_checkpoint_directory=model_save_dir, model_checkpoint_filename=best_model_name)\n",
    "    model.save(os.path.join(model_save_dir, final_model_name))\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    if visualize:\n",
    "        ## Plot Training Progress\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()\n",
    "    \n",
    "    best_model = tf.keras.models.load_model(os.path.join(model_save_dir, best_model_name))\n",
    "    return model, best_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377506b4-c810-4e07-95ff-dca898440a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model_save_dir, model_type, num_samples=3, visualize=False):\n",
    "    results = []\n",
    "    \n",
    "    # Define hyperparameter grids for each model\n",
    "    if model_type == \"fcn\":\n",
    "        model_params_grid = {\n",
    "        'num_hidden_layers': [0, 2, 4, 6, 8 , 10],\n",
    "        'hidden_layer_num_filters': [8, 16, 32, 64, 182, 256]\n",
    "        }\n",
    "        fixed_model_params = {\n",
    "        'first_layer_num_filters': 32,\n",
    "        'first_layer_kernel_size': 3,\n",
    "        'hidden_layer_kernel_size': 3,\n",
    "        'layer_activation': 'sigmoid'\n",
    "        }\n",
    "    elif model_type == \"unet\":\n",
    "        model_params_grid = {\n",
    "            'unet_depth': [2, 3, 4],\n",
    "            'num_filters_base': [4, 8, 16],\n",
    "            'conv_block_size': [2, 4, 6]\n",
    "        }\n",
    "        fixed_model_params = {\n",
    "            'do_batchnorm': True,\n",
    "            'activation': \"relu\"\n",
    "        }\n",
    "    elif model_type == \"resnet\":\n",
    "        model_params_grid = {\n",
    "            'num_filters': [16, 64],\n",
    "            'block_layers': [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]]\n",
    "        }\n",
    "        fixed_model_params = {}\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type. Choose 'fcn', 'unet', or 'resnet'.\")\n",
    "\n",
    "    psnr_values, ssim_values, mse_values = [], [], []\n",
    "    model_params_combinations = []\n",
    "    \n",
    "    grid_combinations = list(itertools.product(*model_params_grid.values()))\n",
    "    \n",
    "    for combination in grid_combinations:\n",
    "        model_params = {**fixed_model_params}\n",
    "        for i, key in enumerate(model_params_grid.keys()):\n",
    "            model_params[key] = combination[i]\n",
    "\n",
    "        print(f\"Processing {model_type} model with params: {model_params}\")\n",
    "        psnr_list, ssim_list, mse_list = [], [], []\n",
    "        \n",
    "        if model_type == \"unet\":\n",
    "            glob_pattern = (f\"unet_bs{32}_lr{1e-4}_depth{model_params['unet_depth']}_\"\n",
    "                            f\"basefilters{model_params['num_filters_base']}_cblocksize{model_params['conv_block_size']}_\"\n",
    "                            f\"BN{model_params['do_batchnorm']}_activ{model_params['activation']}_*_final.keras\")\n",
    "        elif model_type == \"fcn\":\n",
    "            glob_pattern = (f\"fcm_bs{32}_lr{1e-4}_layers{model_params['num_hidden_layers']}_\"\n",
    "                            f\"filter1{32}_size1{3}_filterN{model_params['hidden_layer_num_filters']}_\"\n",
    "                            f\"sizeN{3}_activ{model_params['layer_activation']}_*_final.keras\")\n",
    "        elif model_type == \"resnet\":\n",
    "            block_layers_string = '_'.join(map(str, model_params['block_layers']))\n",
    "            glob_pattern = (f\"resnet_bs{32}_lr{1e-4}_num_filters{model_params['num_filters']}_\"\n",
    "                            f\"block_layers{block_layers_string}_*_final.keras\")\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model type.\")\n",
    "            \n",
    "        matching_files = glob(os.path.join(model_save_dir, glob_pattern))\n",
    "        \n",
    "        if len(matching_files) == 0:\n",
    "            # No saved models exist for this combination, train all runs\n",
    "            print(f\"No pre-trained models found for params {model_params}. Training {num_samples} runs.\")\n",
    "            for i in range(num_samples):\n",
    "                model, best_model, history = do_model_training(\n",
    "                    model=None, \n",
    "                    model_type=model_type, \n",
    "                    fcn_model_params=model_params if model_type == \"fcn\" else None,\n",
    "                    unet_model_params=model_params if model_type == \"unet\" else None,\n",
    "                    resnet_model_params=model_params if model_type == \"resnet\" else None, \n",
    "                    lr=1e-4, \n",
    "                    model_save_dir=model_save_dir, \n",
    "                    visualize=visualize\n",
    "                )\n",
    "\n",
    "                # Collect metrics for each run\n",
    "                avg_psnr, avg_ssim, avg_mse = test_and_score_model(model, num_samples=1, verbose=True)\n",
    "                psnr_list.append(avg_psnr)\n",
    "                ssim_list.append(avg_ssim)\n",
    "                mse_list.append(avg_mse)\n",
    "        else:\n",
    "            # Use existing models if they exist\n",
    "            print(f\"Found {len(matching_files)} pre-trained models for params {model_params}.\")\n",
    "            for i in range(num_samples):\n",
    "                if i < len(matching_files):\n",
    "                    # Load an existing model\n",
    "                    model_path = matching_files[i]\n",
    "                    print(f\"Loading existing model for run {i+1}: {model_path}\")\n",
    "                    model = tf.keras.models.load_model(model_path)\n",
    "                else:\n",
    "                    # Train a new model for missing runs\n",
    "                    print(f\"Training model for missing run {i+1} with params: {model_params}\")\n",
    "                    model, best_model, history = do_model_training(\n",
    "                        model=None, \n",
    "                        model_type=model_type, \n",
    "                        fcn_model_params=model_params if model_type == \"fcn\" else None,\n",
    "                        unet_model_params=model_params if model_type == \"unet\" else None,\n",
    "                        resnet_model_params=model_params if model_type == \"resnet\" else None, \n",
    "                        lr=1e-4, \n",
    "                        model_save_dir=model_save_dir, \n",
    "                        visualize=visualize\n",
    "                    )\n",
    "\n",
    "                # Collect metrics for each run\n",
    "                avg_psnr, avg_ssim, avg_mse = test_and_score_model(model, num_samples=1, verbose=True)\n",
    "                psnr_list.append(avg_psnr)\n",
    "                ssim_list.append(avg_ssim)\n",
    "                mse_list.append(avg_mse)\n",
    "            \n",
    "        # Compute average metrics after 3 runs\n",
    "        avg_psnr = sum(psnr_list) / num_samples\n",
    "        avg_ssim = sum(ssim_list) / num_samples\n",
    "        avg_mse = sum(mse_list) / num_samples\n",
    "\n",
    "        # Append results for this combination of hyperparameters\n",
    "        psnr_values.append(avg_psnr)\n",
    "        ssim_values.append(avg_ssim)\n",
    "        mse_values.append(avg_mse)\n",
    "        model_params_combinations.append(model_params)\n",
    "\n",
    "    # Log the results in a DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'model_type': [model_type] * len(model_params_combinations),\n",
    "        'hyperparameters': [str(params) for params in model_params_combinations],\n",
    "        'PSNR': psnr_values,\n",
    "        'SSIM': ssim_values,\n",
    "        'MSE': mse_values\n",
    "    })\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    results_df.to_csv(os.path.join(model_save_dir, f'{model_type}_grid_search_results.csv'), index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab32ea7b-e91f-4599-8774-be28bc813f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_save_dir = \"./saved_models/grid\"\n",
    "fcn_results_df = grid_search(model_save_dir, model_type=\"fcn\", num_samples=3, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8629b76-166f-4775-83ec-d9bd89b17e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_results_df = grid_search(model_save_dir, model_type=\"unet\", num_samples=3, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17238f-0da4-4602-9f73-7f23fcb5b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_results_df = grid_search(model_save_dir, model_type=\"resnet\", num_samples=3, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
