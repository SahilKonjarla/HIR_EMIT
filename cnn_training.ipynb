{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda5237b-bff8-4700-a414-6eea9e494e39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking if the kernel is working\n"
     ]
    }
   ],
   "source": [
    "print(\"checking if the kernel is working\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8682e60d-fb8a-4d75-a681-2d3b6267d982",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip uninstall -y numpy --quiet\n",
    "!pip uninstall -y tensorflow --quiet\n",
    "!pip install tensorflow==2.17 --quiet\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba89558-8e61-4792-bc8d-57173dda4fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 01:14:12.325027: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 01:14:12.603898: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 01:14:12.676048: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 01:14:12.975833: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0 must be 2.17.0\n",
      "1.26.4 must be 1.26.4\n",
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733274858.082400   16454 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1733274858.298892   16454 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1733274858.299313   16454 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__, \"must be 2.17.0\")\n",
    "import numpy as np\n",
    "print(np.__version__, \"must be 1.26.4\")\n",
    "#from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "import random, os, sys\n",
    "from glob import glob\n",
    "\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741009d-f3df-4b9d-bf8f-52b07f2b30de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2fef53c-16aa-4bee-b751-5b28b7804079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def average_wavelengths(chip, wavelength_indices):\n",
    "    out_chip = chip[:,:,wavelength_indices]\n",
    "    out_chip = np.nanmean(out_chip, axis=2)\n",
    "    return out_chip\n",
    "\n",
    "def convert_chip_to_different_spectra(chip, in_wavelengths, out_wavelengths):\n",
    "    out_chip = []\n",
    "    for out_wavelength_index in range(len(out_wavelengths)):\n",
    "        out_wavelength_start = out_wavelengths[out_wavelength_index][0]\n",
    "        out_wavelength_end = out_wavelengths[out_wavelength_index][1]\n",
    "        \n",
    "        in_wavelengths_to_grab = []\n",
    "        for i in range(len(in_wavelengths)):\n",
    "            in_wavelength = in_wavelengths[i]\n",
    "            if in_wavelength > out_wavelength_start and in_wavelength < out_wavelength_end:\n",
    "                in_wavelengths_to_grab.append(i)\n",
    "        \n",
    "        new_chip_at_wavelegnth = average_wavelengths(chip, in_wavelengths_to_grab)\n",
    "        #print(new_chip_at_wavelegnth.shape)\n",
    "        out_chip.append(new_chip_at_wavelegnth)\n",
    "        \n",
    "    out_chip = np.asarray(out_chip)\n",
    "    out_chip = np.moveaxis(out_chip,0,2)\n",
    "    #print(out_chip.shape)\n",
    "    return out_chip\n",
    "\n",
    "def convert_emit_chip_to_planetscope_chip(emit_chip_filename, planetscope_chip_filename, emit_wavelengths, planetscope_wavelengths):\n",
    "    in_chip = np.load(emit_chip_filename)\n",
    "    out_chip = convert_chip_to_different_spectra(in_chip, emit_wavelengths, planetscope_wavelengths)\n",
    "    np.save(planetscope_chip_filename, out_chip)\n",
    "    \n",
    "def convert_whole_dataset(emit_chip_directory, planetscope_chip_directory, emit_wavelengths, planetscope_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=0):\n",
    "    if not os.path.exists(planetscope_chip_directory):\n",
    "        os.mkdir(planetscope_chip_directory)\n",
    "        \n",
    "    in_chip_paths = glob(os.path.join(emit_chip_directory,'*'))\n",
    "    for chip_index, in_chip_path in enumerate(in_chip_paths):\n",
    "        out_filename = chipname_prefix + f\"{chipname_start_index+chip_index:05}\" + \".npy\"\n",
    "        out_path = os.path.join(planetscope_chip_directory, out_filename)\n",
    "        \n",
    "        convert_emit_chip_to_planetscope_chip(in_chip_path, out_path, emit_wavelengths, planetscope_wavelengths)\n",
    "            \n",
    "        \n",
    "        \n",
    "#Src: https://assets.planet.com/docs/Planet_PSScene_Imagery_Product_Spec_letter_screen.pdf Pg 10\n",
    "planetscope_old_wavelengths = [(455, 515), (500, 590), (590, 670), (780,860)]\n",
    "planetscope_new_wavelengths = [(431,452), (465,515), (513,549), (547,583), (600,620), (650,680), (697,713), (845,885)]\n",
    "        \n",
    "emit_wavelengths = [ 381.00558,  388.4092 ,  395.81583,  403.2254 ,  410.638  ,\n",
    "        418.0536 ,  425.47214,  432.8927 ,  440.31726,  447.7428 ,\n",
    "        455.17035,  462.59888,  470.0304 ,  477.46292,  484.89743,\n",
    "        492.33292,  499.77142,  507.2099 ,  514.6504 ,  522.0909 ,\n",
    "        529.5333 ,  536.9768 ,  544.42126,  551.8667 ,  559.3142 ,\n",
    "        566.7616 ,  574.20905,  581.6585 ,  589.108  ,  596.55835,\n",
    "        604.0098 ,  611.4622 ,  618.9146 ,  626.36804,  633.8215 ,\n",
    "        641.2759 ,  648.7303 ,  656.1857 ,  663.6411 ,  671.09753,\n",
    "        678.5539 ,  686.0103 ,  693.4677 ,  700.9251 ,  708.38354,\n",
    "        715.84094,  723.2993 ,  730.7587 ,  738.2171 ,  745.6765 ,\n",
    "        753.1359 ,  760.5963 ,  768.0557 ,  775.5161 ,  782.97754,\n",
    "        790.4379 ,  797.89935,  805.36176,  812.8232 ,  820.2846 ,\n",
    "        827.746  ,  835.2074 ,  842.66986,  850.1313 ,  857.5937 ,\n",
    "        865.0551 ,  872.5176 ,  879.98004,  887.44147,  894.90393,\n",
    "        902.3664 ,  909.82886,  917.2913 ,  924.7538 ,  932.21625,\n",
    "        939.6788 ,  947.14026,  954.6027 ,  962.0643 ,  969.5268 ,\n",
    "        976.9883 ,  984.4498 ,  991.9114 ,  999.37286, 1006.8344 ,\n",
    "       1014.295  , 1021.7566 , 1029.2172 , 1036.6777 , 1044.1383 ,\n",
    "       1051.5989 , 1059.0596 , 1066.5201 , 1073.9797 , 1081.4404 ,\n",
    "       1088.9    , 1096.3597 , 1103.8184 , 1111.2781 , 1118.7368 ,\n",
    "       1126.1964 , 1133.6552 , 1141.1129 , 1148.5717 , 1156.0304 ,\n",
    "       1163.4882 , 1170.9459 , 1178.4037 , 1185.8616 , 1193.3184 ,\n",
    "       1200.7761 , 1208.233  , 1215.6898 , 1223.1467 , 1230.6036 ,\n",
    "       1238.0596 , 1245.5154 , 1252.9724 , 1260.4283 , 1267.8833 ,\n",
    "       1275.3392 , 1282.7942 , 1290.2502 , 1297.7052 , 1305.1603 ,\n",
    "       1312.6144 , 1320.0685 , 1327.5225 , 1334.9756 , 1342.4287 ,\n",
    "       1349.8818 , 1357.3351 , 1364.7872 , 1372.2384 , 1379.6907 ,\n",
    "       1387.1418 , 1394.5931 , 1402.0433 , 1409.4937 , 1416.944  ,\n",
    "       1424.3933 , 1431.8427 , 1439.292  , 1446.7404 , 1454.1888 ,\n",
    "       1461.6372 , 1469.0847 , 1476.5321 , 1483.9796 , 1491.4261 ,\n",
    "       1498.8727 , 1506.3192 , 1513.7649 , 1521.2104 , 1528.655  ,\n",
    "       1536.1007 , 1543.5454 , 1550.9891 , 1558.4329 , 1565.8766 ,\n",
    "       1573.3193 , 1580.7621 , 1588.205  , 1595.6467 , 1603.0886 ,\n",
    "       1610.5295 , 1617.9705 , 1625.4104 , 1632.8513 , 1640.2903 ,\n",
    "       1647.7303 , 1655.1694 , 1662.6074 , 1670.0455 , 1677.4836 ,\n",
    "       1684.9209 , 1692.358  , 1699.7952 , 1707.2314 , 1714.6667 ,\n",
    "       1722.103  , 1729.5383 , 1736.9727 , 1744.4071 , 1751.8414 ,\n",
    "       1759.2749 , 1766.7084 , 1774.1418 , 1781.5743 , 1789.007  ,\n",
    "       1796.4385 , 1803.8701 , 1811.3008 , 1818.7314 , 1826.1611 ,\n",
    "       1833.591  , 1841.0206 , 1848.4495 , 1855.8773 , 1863.3052 ,\n",
    "       1870.733  , 1878.16   , 1885.5869 , 1893.013  , 1900.439  ,\n",
    "       1907.864  , 1915.2892 , 1922.7133 , 1930.1375 , 1937.5607 ,\n",
    "       1944.9839 , 1952.4071 , 1959.8295 , 1967.2518 , 1974.6732 ,\n",
    "       1982.0946 , 1989.515  , 1996.9355 , 2004.355  , 2011.7745 ,\n",
    "       2019.1931 , 2026.6118 , 2034.0304 , 2041.4471 , 2048.865  ,\n",
    "       2056.2808 , 2063.6965 , 2071.1123 , 2078.5273 , 2085.9421 ,\n",
    "       2093.3562 , 2100.769  , 2108.1821 , 2115.5942 , 2123.0063 ,\n",
    "       2130.4175 , 2137.8289 , 2145.239  , 2152.6482 , 2160.0576 ,\n",
    "       2167.467  , 2174.8755 , 2182.283  , 2189.6904 , 2197.097  ,\n",
    "       2204.5034 , 2211.9092 , 2219.3147 , 2226.7195 , 2234.1233 ,\n",
    "       2241.5269 , 2248.9297 , 2256.3328 , 2263.7346 , 2271.1365 ,\n",
    "       2278.5376 , 2285.9387 , 2293.3386 , 2300.7378 , 2308.136  ,\n",
    "       2315.5342 , 2322.9326 , 2330.3298 , 2337.7263 , 2345.1216 ,\n",
    "       2352.517  , 2359.9126 , 2367.3071 , 2374.7007 , 2382.0935 ,\n",
    "       2389.486  , 2396.878  , 2404.2695 , 2411.6604 , 2419.0513 ,\n",
    "       2426.4402 , 2433.8303 , 2441.2183 , 2448.6064 , 2455.9944 ,\n",
    "       2463.3816 , 2470]\n",
    "    \n",
    "emit_dataset_path_0 = \"./reflectance/\"\n",
    "emit_dataset_path_1 = \"./reflectance_1/\"\n",
    "emit_dataset_path_2 = \"./reflectance_2/\"\n",
    "emit_dataset_path_3 = \"./reflectance_3/\"\n",
    "emit_dataset_path_4 = \"./reflectance_4/\"\n",
    "emit_dataset_path_5 = \"./reflectance_5/\"\n",
    "\n",
    "\n",
    "# planetscope_old_dataset_path = \"./dataset_for_cnn/train/x/\"\n",
    "# planetscope_new_dataset_path = \"./dataset_for_cnn/train/y/\"\n",
    "\n",
    "if False: #manual toggle since this only needs to happen once\n",
    "    #Granule 0:\n",
    "    print(\"Processing granule 0\")\n",
    "    convert_whole_dataset(emit_dataset_path_0, \"./dataset_for_cnn/train/x/\", emit_wavelengths, planetscope_old_wavelengths, chipname_prefix = \"chip_\")\n",
    "    convert_whole_dataset(emit_dataset_path_0, \"./dataset_for_cnn/train/y/\", emit_wavelengths, planetscope_new_wavelengths, chipname_prefix = \"chip_\")\n",
    "    print(\"Granule 0 processed into train data\")\n",
    "\n",
    "    #Granule 1:\n",
    "    print(\"Processing granule 1\")\n",
    "    convert_whole_dataset(emit_dataset_path_1, \"./dataset_for_cnn/test/x/\", emit_wavelengths, planetscope_old_wavelengths, chipname_prefix = \"chip_\")\n",
    "    convert_whole_dataset(emit_dataset_path_1, \"./dataset_for_cnn/test/y/\", emit_wavelengths, planetscope_new_wavelengths, chipname_prefix = \"chip_\")\n",
    "    print(\"Granule 1 processed into test data\")\n",
    "\n",
    "    # #Granule 2:\n",
    "    # convert_whole_dataset(emit_dataset_path_2, \"./dataset_for_cnn/valid/x/\", emit_wavelengths, planetscope_old_wavelengths, chipname_prefix = \"chip_\")\n",
    "    # convert_whole_dataset(emit_dataset_path_2, \"./dataset_for_cnn/valid/y/\", emit_wavelengths, planetscope_new_wavelengths, chipname_prefix = \"chip_\")\n",
    "\n",
    "    # #Granule 3:\n",
    "    # convert_whole_dataset(emit_dataset_path_3, \"./dataset_for_cnn/train/x/\", emit_wavelengths, planetscope_old_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=10000)\n",
    "    # convert_whole_dataset(emit_dataset_path_3, \"./dataset_for_cnn/train/y/\", emit_wavelengths, planetscope_new_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=10000)\n",
    "\n",
    "    # #Granule 4:\n",
    "    # convert_whole_dataset(emit_dataset_path_4, \"./dataset_for_cnn/train/x/\", emit_wavelengths, planetscope_old_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=20000)\n",
    "    # convert_whole_dataset(emit_dataset_path_4, \"./dataset_for_cnn/train/y/\", emit_wavelengths, planetscope_new_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=20000)\n",
    "\n",
    "    # #Granule 5:\n",
    "    # convert_whole_dataset(emit_dataset_path_5, \"./dataset_for_cnn/train/x/\", emit_wavelengths, planetscope_old_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=30000)\n",
    "    # convert_whole_dataset(emit_dataset_path_5, \"./dataset_for_cnn/train/y/\", emit_wavelengths, planetscope_new_wavelengths, chipname_prefix = \"chip_\", chipname_start_index=30000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb4d0e0-ef93-46b6-a19d-4ec1ae63acac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, x_chip_path_dir, y_chip_path_dir, batch_size, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.x_chip_path_dir = x_chip_path_dir\n",
    "        self.y_chip_path_dir = y_chip_path_dir\n",
    "        \n",
    "        self.x_chip_path_list = glob(os.path.join(self.x_chip_path_dir,'*'))\n",
    "        self.y_chip_path_list = glob(os.path.join(self.y_chip_path_dir,'*'))\n",
    "        \n",
    "        self.num_files = len(self.x_chip_path_list)\n",
    "        self.index_path = list(range(self.num_files))+list(range(self.num_files))\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.index_path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in range(self.batch_size):\n",
    "            x = np.load(self.x_chip_path_list[self.index_path[index]])\n",
    "            y = np.load(self.y_chip_path_list[self.index_path[index]])\n",
    "            \n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "        \n",
    "        X = np.asarray(X).astype('float32')\n",
    "        Y = np.asarray(Y).astype('float32')\n",
    "       \n",
    "        \n",
    "       # # print(np.isnan(np.min(X)))\n",
    "       #  x = np.asarray(x).astype('float32')\n",
    "       #  x = np.zeros((1, 64, 64, 4))\n",
    "       #  y = np.zeros((1,64,64,8))\n",
    "       #  print(x)\n",
    "        # X = tf.convert_to_tensor(X.tolist())\n",
    "        # Y = tf.convert_to_tensor(Y.tolist())\n",
    "        \n",
    "        return X, Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_files // self.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f58bea20-e21a-4eb5-a889-fcb493a080f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_fcn_model(num_hidden_layers=3, first_layer_num_filters=32, first_layer_kernel_size=3, hidden_layer_num_filters=64, hidden_layer_kernel_size=3, layer_activation='sigmoid'):\n",
    "    # num_hidden_layers must be > 1\n",
    "    # *_num_filters must be positive integer - recommend 8, 16, 32, 64, etc\n",
    "    # *_kernel_size must be odd positive integer - recommend 1, 3, 5, 7, 9, etc\n",
    "    # see documentation for tf.keras for different activations\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(first_layer_num_filters, (first_layer_kernel_size, first_layer_kernel_size), activation=layer_activation, padding='same'))\n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(tf.keras.layers.Conv2D(hidden_layer_num_filters, (hidden_layer_kernel_size, hidden_layer_kernel_size), activation=layer_activation, padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='sigmoid', padding='same'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "#src: https://github.com/oekosheri/tensorflow_unet_scaling/blob/main/models.py\n",
    "def conv_block(input, num_filters, conv_block_size=2, do_batchnorm=True, activation=\"relu\"):\n",
    "    x = tf.keras.layers.Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    if do_batchnorm == True:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)  # Not in the original network.\n",
    "    x = tf.keras.layers.Activation(activation)(x)\n",
    "    \n",
    "    for i in range(conv_block_size-1):\n",
    "        x = tf.keras.layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "        if do_batchnorm == True:\n",
    "            x = tf.keras.layers.BatchNormalization()(x)  # Not in the original network\n",
    "        x = tf.keras.layers.Activation(activation)(x)\n",
    "        \n",
    "    return x\n",
    "\n",
    "def encoder_block(input, num_filters, conv_block_size=2, do_batchnorm=True, activation=\"relu\"):\n",
    "    x = conv_block(input, num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "    p = tf.keras.layers.MaxPool2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters, conv_block_size=2, do_batchnorm=True, activation=\"relu\"):\n",
    "    \n",
    "    x = tf.keras.layers.Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "    return x\n",
    "\n",
    "def generate_unet_model(unet_depth=3, num_filters_base=8, conv_block_size=2, do_batchnorm=True, activation=\"relu\"):\n",
    "    inputs = tf.keras.layers.Input((64,64,4)) #Assuming 64x64x4 chipsize\n",
    "    \n",
    "    s_list = []\n",
    "    p_list = []\n",
    "    d_list = []\n",
    "    num_filters = num_filters_base    \n",
    "    \n",
    "    #encoding steps\n",
    "    for i in range(unet_depth):\n",
    "        if i == 0:\n",
    "            s, p = encoder_block(inputs, num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "        else:\n",
    "            s, p = encoder_block(p_list[i-1], num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "        s_list.append(s)\n",
    "        p_list.append(p)\n",
    "        num_filters *= 2\n",
    "        \n",
    "    #bridge\n",
    "    b1 = conv_block(p_list[-1], num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "    num_filters = int(num_filters/2)\n",
    "    \n",
    "    #decoding steps:\n",
    "    for i in range(unet_depth):\n",
    "        if i == 0:\n",
    "            d = decoder_block(b1, s_list[-1-i], num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "        else:\n",
    "            d = decoder_block(d_list[i-1], s_list[-1-i], num_filters, conv_block_size=conv_block_size, do_batchnorm=do_batchnorm, activation=activation)\n",
    "            \n",
    "        d_list.append(d)\n",
    "        num_filters = int(num_filters/2)\n",
    "    \n",
    "    #head\n",
    "    output = tf.keras.layers.Conv2D(8, (3, 3), activation=activation, padding='same')(d_list[-1])\n",
    "    model = tf.keras.models.Model(inputs, output, name=\"U-Net\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2bf54b58-bdf8-4319-bfdb-efcc525bab08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, batch_size=1, num_epochs=10, learning_rate=1e-3):\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=tf.keras.losses.MeanSquaredError(), metrics=['accuracy', 'root_mean_squared_error'])\n",
    "    #print(model.summary())\n",
    "    training_generator = CustomDataGenerator(\"./dataset_for_cnn/train/x/\", \"./dataset_for_cnn/train/y/\", batch_size)\n",
    "    model.fit(training_generator, epochs = num_epochs)\n",
    "    return model\n",
    "    \n",
    "def score_one_datapoint(y, y_pred):\n",
    "    psnr = skimage.metrics.peak_signal_noise_ratio(np.asarray(y)[0,:,:,:], y_pred[0,:,:,:])\n",
    "    ssim = skimage.metrics.structural_similarity(np.asarray(y)[0,:,:,:], y_pred[0,:,:,:], data_range = 1)\n",
    "    mse = skimage.metrics.mean_squared_error(np.asarray(y)[0,:,:,:], y_pred[0,:,:,:])\n",
    "    return psnr, ssim, mse\n",
    "    \n",
    "def test_and_score_model(model, verbose=False, num_samples=np.inf, debug=False):\n",
    "    testing_generator = CustomDataGenerator(\"./dataset_for_cnn/test/x/\", \"./dataset_for_cnn/test/y/\", 1)\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    mse_list = []\n",
    "    if verbose:\n",
    "        print(\"Testing and Scoring Model...\")\n",
    "    for i in range(min(testing_generator.__len__(), num_samples)):\n",
    "        x, y = testing_generator.__getitem__(i)\n",
    "        y_pred = model.predict(x, steps=1, verbose=0)\n",
    "        psnr, ssim, mse = score_one_datapoint(y, y_pred)\n",
    "        psnr_list.append(psnr)\n",
    "        ssim_list.append(ssim)\n",
    "        mse_list.append(mse)\n",
    "    \n",
    "    avg_psnr = np.mean(np.asarray(psnr_list))\n",
    "    avg_ssim = np.mean(np.asarray(ssim_list))\n",
    "    avg_mse = np.mean(np.asarray(mse_list))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Average PSNR:\", avg_psnr)\n",
    "        print(\"Average SSIM:\", avg_ssim)\n",
    "        print(\"Average MSE:\", avg_mse)\n",
    "        \n",
    "    if debug:\n",
    "        return psnr_list, ssim_list, mse_list\n",
    "    else:\n",
    "        return avg_psnr, avg_ssim, avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc72b3b4-f9a9-4148-afbb-ddabc5eda7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - accuracy: 0.2423 - loss: 0.0462 - root_mean_squared_error: 0.2110\n",
      "Epoch 2/10\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.3432 - loss: 0.0087 - root_mean_squared_error: 0.0929\n",
      "Epoch 3/10\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.4977 - loss: 0.0060 - root_mean_squared_error: 0.0771\n",
      "Epoch 4/10\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.6047 - loss: 0.0034 - root_mean_squared_error: 0.0583\n",
      "Epoch 5/10\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.6883 - loss: 0.0032 - root_mean_squared_error: 0.0562\n",
      "Epoch 6/10\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7460 - loss: 0.0025 - root_mean_squared_error: 0.0495\n",
      "Epoch 7/10\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7578 - loss: 0.0018 - root_mean_squared_error: 0.0428\n",
      "Epoch 8/10\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7600 - loss: 0.0038 - root_mean_squared_error: 0.0609\n",
      "Epoch 9/10\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7794 - loss: 0.0014 - root_mean_squared_error: 0.0378\n",
      "Epoch 10/10\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7952 - loss: 0.0016 - root_mean_squared_error: 0.0395\n"
     ]
    }
   ],
   "source": [
    "## Training Parameters\n",
    "#TODO: Play with these values first to see if you can get models to converge quickly\n",
    "batch_size = 1              #Normally you want to push this as high as you can, multiples of 2, until you run out of VRAM\n",
    "num_epochs = 10             #Increase to train more.  May need to be increased as batch size is increased to keep number of training steps equal\n",
    "learning_rate = 1e-3        #Play with this to get models to converge faster\n",
    "\n",
    "## FCN Model Parameters\n",
    "#TODO: Grid-Search these, log PSNR, SSIM, MSE for each trained model in a spreadsheet:\n",
    "num_hidden_layers=3         #try 0,1,2,3,4,5,...,12\n",
    "first_layer_num_filters=32  #try 4,8,16,32,64,128\n",
    "first_layer_kernel_size=3   #try 1,3,5,7,9 must be odd\n",
    "hidden_layer_num_filters=64 #try 4,8,16,32,64,128\n",
    "hidden_layer_kernel_size=3  #try 1,3,5,7,9 must be odd\n",
    "layer_activation='sigmoid'  #try sigmoid and relu\n",
    "\n",
    "## U-Net Model Parameters\n",
    "#TODO: Grid-Search these, log PSNR, SSIM, MSE for each trained model in a spreadsheet:\n",
    "unet_depth=3             #try 1,2,3,4,5,6,7,8\n",
    "num_filters_base=8       #try 4,8,16,32\n",
    "conv_block_size=2        #try 1,2,3,4,5,6,7,8\n",
    "do_batchnorm=True        #try true/false\n",
    "activation=\"relu\"        #try sigmoid and relu\n",
    "\n",
    "## Generate FCM\n",
    "# model = generate_fcn_model(num_hidden_layers=num_hidden_layers, \n",
    "#                            first_layer_num_filters=first_layer_num_filters, \n",
    "#                            first_layer_kernel_size=first_layer_kernel_size, \n",
    "#                            hidden_layer_num_filters=hidden_layer_num_filters, \n",
    "#                            hidden_layer_kernel_size=hidden_layer_kernel_size,\n",
    "#                            layer_activation=layer_activation)\n",
    "\n",
    "## Generate U-Net\n",
    "model = generate_unet_model(unet_depth=unet_depth, \n",
    "                            num_filters_base=num_filters_base, \n",
    "                            conv_block_size=conv_block_size, \n",
    "                            do_batchnorm=do_batchnorm, \n",
    "                            activation=activation)\n",
    "\n",
    "\n",
    "## Train Model\n",
    "model = train_model(model, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55bfc088-899e-4eec-962c-4445cd2bda74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing and Scoring Model...\n",
      "Average PSNR: 31.62634021957396\n",
      "Average SSIM: 0.8623859927359433\n",
      "Average MSE: 0.0006904384368934478\n"
     ]
    }
   ],
   "source": [
    "## Test Model\n",
    "avg_psnr, avg_ssim, avg_mse = test_and_score_model(model, verbose=True, num_samples=10, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8404e3a8-e761-4d0b-9c7b-42e556da2a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.62634021957396"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c201d22c-f07e-41e6-90e1-574dfb9dfc5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8623859927359433"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0832a63b-a784-40b7-8ab6-ff251309b49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006904384368934478"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a510d4e1-48f2-4172-8d39-f870a0994899",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratch ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "707b1a95-0566-4753-b73e-61eece33d4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = [0,1,2]\n",
    "i[-0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a249d0a-4787-4dfe-90ac-d82f4ef95099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377506b4-c810-4e07-95ff-dca898440a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018aa523-38e5-477f-b48b-5581ea32970c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7bfecb-1960-4eb8-8bb1-87dc48a8021c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d5fee-fc9a-41f7-86ce-e43f99dacd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692ca77-6695-40fe-a431-bf7a59cf29d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51db87-b381-48d7-81c5-b35a7ddc21fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataGenerator2(tf.keras.utils.Sequence):\n",
    "#     def __init__(self, x_chip_path_dir, y_chip_path_dir, batch_size, shuffle=True):\n",
    "#         self.x_chip_path_dir = x_chip_path_dir\n",
    "#         self.y_chip_path_dir = y_chip_path_dir\n",
    "\n",
    "#         # List all files in the directories\n",
    "#         self.x_chip_path_list = glob(os.path.join(self.x_chip_path_dir, '*'))\n",
    "#         self.y_chip_path_list = glob(os.path.join(self.y_chip_path_dir, '*'))\n",
    "\n",
    "#         self.num_files = len(self.x_chip_path_list)\n",
    "#         assert len(self.x_chip_path_list) == len(self.y_chip_path_list), \\\n",
    "#             \"Mismatch between X and Y files.\"\n",
    "\n",
    "#         self.index_path = np.arange(self.num_files)\n",
    "#         self.batch_size = batch_size\n",
    "#         self.shuffle = shuffle\n",
    "#         self.on_epoch_end()\n",
    "\n",
    "#     def on_epoch_end(self):\n",
    "#         \"\"\"Shuffle data at the end of each epoch.\"\"\"\n",
    "#         if self.shuffle:\n",
    "#             np.random.shuffle(self.index_path)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Number of batches per epoch.\"\"\"\n",
    "#         return int(np.floor(self.num_files / self.batch_size))\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         \"\"\"Generate one batch of data.\"\"\"\n",
    "#         # Indices for this batch\n",
    "#         batch_indices = self.index_path[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "#         # Initialize arrays\n",
    "#         X, Y = [], []\n",
    "#         for idx in batch_indices:\n",
    "#             x = np.load(self.x_chip_path_list[idx])\n",
    "#             y = np.load(self.y_chip_path_list[idx])\n",
    "\n",
    "#             # Ensure shapes match model requirements\n",
    "#             X.append(x)\n",
    "#             Y.append(y)\n",
    "\n",
    "#         # Convert lists to NumPy arrays\n",
    "#         X = np.asarray(X, dtype=np.float32)  # Ensure compatible dtype\n",
    "#         Y = np.asarray(Y, dtype=np.float32)\n",
    "        \n",
    "#         print(\"X shape:\", X.shape)\n",
    "#         print(\"Y shape:\", Y.shape)\n",
    "#         print(\"X dtype:\", X.dtype)\n",
    "#         print(\"Y dtype:\", Y.dtype)\n",
    "\n",
    "#         # Ensure that X and Y are 4D arrays (for Conv2D, for example)\n",
    "#         # X shape should be (batch_size, height, width, channels), if not already\n",
    "#         if X.ndim == 3:  # If it is 3D, add a channels dimension (e.g., grayscale image)\n",
    "#             X = np.expand_dims(X, axis=-1)  # Convert shape to (batch_size, height, width, 1)\n",
    "        \n",
    "#         # Check for nested structures or unexpected object types inside X or Y\n",
    "#         print(f\"X is a numpy array: {isinstance(X, np.ndarray)}\")\n",
    "#         print(f\"Y is a numpy array: {isinstance(Y, np.ndarray)}\")\n",
    "\n",
    "#         # Check if the arrays are consistent and fully flattened\n",
    "#         try:\n",
    "#             X = np.array(X, dtype=np.float32)\n",
    "#             Y = np.array(Y, dtype=np.float32)\n",
    "\n",
    "#             # Debug: check the contents of X and Y\n",
    "#             print(f\"X[0]: {X[0]}\")\n",
    "#             print(f\"Y[0]: {Y[0]}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error while checking array structure: {e}\")\n",
    "\n",
    "#         # Try converting to TensorFlow tensors\n",
    "#         try:\n",
    "#             X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "#             Y_tensor = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "\n",
    "#             print(f\"X tensor shape: {X_tensor.shape}\")\n",
    "#             print(f\"Y tensor shape: {Y_tensor.shape}\")\n",
    "#         except ValueError as e:\n",
    "#             print(f\"Error converting to tensor: {e}\")\n",
    "#             print(f\"X: {X}\")\n",
    "#             print(f\"Y: {Y}\")\n",
    "#             raise e\n",
    "\n",
    "#         return X_tensor, Y_tensor"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
