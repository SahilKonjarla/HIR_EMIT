{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clone into the EMIT Resources Repository to access to helpful visualization tools, ONLY DO THIS ONCE\n",
    "#!git clone https://github.com/nasa/EMIT-Data-Resources.git\n",
    "#!cd EMIT-Data-Resources/python/modules\n",
    "#!cp EMIT-Data-Resources/python/modules/emit_tools.py /home/jupyter/HIR_EMIT/hyperspectral_image_reconstruction_EMIT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOW4O83Hy6Ai",
    "outputId": "2b11d1ac-f8ba-4863-c602-7b7b9dfadef9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install necessary Python libraries\n",
    "!pip install numpy --quiet\n",
    "!pip install hvplot --quiet\n",
    "!pip install netCDF4 --quiet\n",
    "!pip install spectral --quiet\n",
    "!pip install rasterio --quiet\n",
    "!pip install rioxarray --quiet\n",
    "!pip install cartopy geoviews --quiet\n",
    "!pip install s3fs --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "9FT87donsC12",
    "outputId": "d6dce32c-d5aa-4828-f301-4a864d784bb1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import netCDF4 as nc\n",
    "import math\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('/home/jupyter/HIR_EMIT/EMIT-Data-Resources/python/modules')\n",
    "\n",
    "from emit_tools import emit_xarray\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperspectralImageProcessor:\n",
    "    def __init__(self, input_file, output_file):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        Load in the hyperspectral image and separate 'sensor_band_parameters' and 'location' in order to access any additional data dimeensions we need\n",
    "        (i.e. wavelengths and geographic details).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initially opening file into xarray\n",
    "        ds_nc = nc.Dataset(self.input_file)\n",
    "        ds = xr.open_dataset(self.input_file)\n",
    "        \n",
    "        wvl = xr.open_dataset(file_path, group='sensor_band_parameters')\n",
    "        loc = xr.open_dataset(file_path, group='location')\n",
    "        \n",
    "        # Set the group's data as coordinates to the main dataset (using the spatial variables of downtrack and crosstrack)\n",
    "        ds = ds.assign_coords({'downtrack': ds.downtrack.data,\n",
    "                       'crosstrack': ds.crosstrack.data,\n",
    "                       **wvl.variables,\n",
    "                       **loc.variables})\n",
    "        \n",
    "        # Since these datasets are large, we can go ahead and delete objects we won't be using to conserve memory.\n",
    "        del wvl\n",
    "        del loc\n",
    "        \n",
    "        # Switch dimensions of the dataset from bands to wavelength to provide easier access to our spectral data\n",
    "        ds = ds.swap_dims({'bands': 'wavelengths'})\n",
    "        return ds\n",
    "    \n",
    "    def orthorectify(self):\n",
    "        \"\"\"\n",
    "        Orthorectification of the granule\n",
    "        \"\"\"\n",
    "        ds_geo = emit_xarray(self.input_file, ortho=True)\n",
    "        ds_geo.reflectance.data[ds_geo.reflectance.data == -9999] = np.nan  # Mask invalid values\n",
    "        return ds_geo\n",
    "    \n",
    "    def save_chip_to_disk(self, chip, chip_index):\n",
    "        \"\"\"\n",
    "        Save a single chip to a NetCDF file only if it has valid reflectance data (i.e., not all NaNs).\n",
    "\n",
    "        Parameters:\n",
    "            chip: The chip to be saved (xarray Dataset or DataArray)\n",
    "            chip_index: The index of the chip (for naming the output file)\n",
    "        \"\"\"\n",
    "        # Check if the reflectance data has any valid (non-NaN) values\n",
    "        if chip.reflectance.isnull().any():\n",
    "            print(f\"Chip {chip_index} contains NaN values and will not be saved.\")\n",
    "        else:\n",
    "            output_file = f\"{self.output_file}chip_{chip_index}.nc\"\n",
    "            chip.to_netcdf(output_file)\n",
    "    \n",
    "    def split_into_chips(self, ds_oc, chip_size=(64,64), overlap_percent=10):\n",
    "        \"\"\"\n",
    "        Split the orthorectified hyperspectral image into smaller chips (subsets).\n",
    "        \n",
    "        Parameters:\n",
    "            ds_geo: The orthorectified dataset\n",
    "            chip_size: Tuple (height, width) for the chip size\n",
    "            overlap: Whether to allow overlap between chips (default is False)\n",
    "            \n",
    "        Returns:\n",
    "            List of chips (subsets of the image data)\n",
    "        \"\"\"\n",
    "        lat_height,lon_width = chip_size\n",
    "        chips_saved = 0\n",
    "        \n",
    "        # Calculate step size based on overlap_percent\n",
    "        lat_step = int(lat_height * (1 - overlap_percent / 100))\n",
    "        lon_step = int(lon_width * (1 - overlap_percent / 100))\n",
    "        \n",
    "        data_lat, data_lon, num_wavelength = ds_oc.reflectance.shape\n",
    "        \n",
    "        # Loop over the granule to extract chips\n",
    "        for i in range(0, data_lat, lat_step):\n",
    "            for j in range(0, data_lon, lon_step):\n",
    "                # Ensure the chip fits within the bounds of the image\n",
    "                chip = ds_oc.isel(latitude=slice(i, i+lat_height), longitude=slice(j, j+lon_width))\n",
    "                \n",
    "                # Save the chip to disk\n",
    "                chips_saved += 1\n",
    "                self.save_chip_to_disk(chip, chips_saved)\n",
    "                \n",
    "                # Delete the chip from memory to free up space\n",
    "                del chip\n",
    "        \n",
    "        print(f\"Saved {chips_saved} chips to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# State the location of the granule you are currently looking at and the output directory of where you want your chips to be\n",
    "file_path = \"path to your EMIT data\"\n",
    "output_path = \"path to where you want your chips to be\"\n",
    "processor = HyperspectralImageProcessor(file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment this cell to load, orthorectify, and split the granule into chips\n",
    "ds = processor.load_dataset()\n",
    "ds_oc = processor.orthorectify()\n",
    "processor.split_into_chips(ds_oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperspectralChipProcessor:\n",
    "    def __init__(self, input_dir, reflec_dir):\n",
    "        self.input_dir = input_dir\n",
    "        self.reflec_dir = reflec_dir\n",
    "        \n",
    "        if not os.path.exists(self.reflec_dir):\n",
    "            os.makedirs(self.reflec_dir)\n",
    "\n",
    "    def process_chip(self, file_path):\n",
    "        chip = xr.open_dataset(file_path)\n",
    "        \n",
    "        reflectance = chip.reflectance.values\n",
    "        reflectance_filename = os.path.join(self.reflec_dir, os.path.basename(file_path).replace('.nc', '_reflec.npy'))\n",
    "        np.save(reflectance_filename, reflectance)\n",
    "        print(f\"Saved updated reflectance data for {os.path.basename(file_path)} to {reflectance_filename}\")\n",
    "\n",
    "    def process_all_chips(self):\n",
    "        chip_files = [f for f in os.listdir(self.input_dir) if f.endswith('.nc')]\n",
    "        for chip_file in chip_files:\n",
    "            file_path = os.path.join(self.input_dir, chip_file)\n",
    "            self.process_chip(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# State the chip directory and where you want the reflectance numpy arrays to be\n",
    "reflec_dir = \"path to where you want reflectance data numpy arrays to be\"\n",
    "chip_processor = HyperspectralChipProcessor(output_path, reflec_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process all chips and extract their reflectance data\n",
    "chip_processor.process_all_chips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment this cell, to visualize the entire granule or a singular chip\n",
    "#chip_1 = xr.open_dataset('chips/chip_1000.nc')\n",
    "#chip_1_wavelength = chip_1.sel(wavelengths=385, method='nearest')\n",
    "#chip_1_wavelength.hvplot.image(cmap='viridis', aspect='equal', frame_width=720).opts(title=f\"{chip_1_wavelength.wavelengths.values:.3f} {chip_1_wavelength.wavelengths.units}\")\n",
    "#chip_1.sel(wavelengths=385, method='nearest').hvplot.image(cmap='Viridis', geo=True, tiles='ESRI', alpha=0.8, frame_height=600).opts(\n",
    "#    title=f\"Reflectance at {chip_1_wavelength.wavelengths.values:.3f} {chip_1_wavelength.wavelengths.units} (Orthorectified)\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cmdJu-10USt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
